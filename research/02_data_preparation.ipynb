{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/admin/HB_IDS/HackBeta/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/admin/HB_IDS/HackBeta'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entity\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataPreparationConfig:\n",
    "    root_dir: Path\n",
    "    local_data_file: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hackbeta.constants import *\n",
    "from hackbeta.utils.common import read_yaml,create_directories\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath=CONFIG_FILE_PATH, params_filepath=PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        # Create the root directory for artifacts as defined in the YAML configuration\n",
    "        create_directories([self.config['artifacts_root']])\n",
    "\n",
    "    def get_data_preparation_config(self) -> DataPreparationConfig:\n",
    "        config = self.config['data_preparation']\n",
    "\n",
    "        # Ensure the data ingestion root directory exists\n",
    "        create_directories([config['root_dir']])\n",
    "\n",
    "        # Create a DataIngestionConfig object with paths from the YAML configuration\n",
    "        data_preparation_config = DataPreparationConfig(\n",
    "            root_dir=Path(config['root_dir']),\n",
    "            local_data_file=Path(config['local_data_file']),    # Target path for the moved CSV file\n",
    "        )\n",
    "\n",
    "        return data_preparation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparation:\n",
    "    def __init__(self, config: DataPreparationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Loads the dataset using pandas from the local data file path specified\n",
    "        in the configuration.\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(self.config.local_data_file)\n",
    "        df = self.clean_data(df)\n",
    "        print(df.head(5))\n",
    "        self.save_cleaned_data(df)\n",
    "        return df\n",
    "\n",
    "    def clean_data(self, df):\n",
    "        \"\"\"\n",
    "        Cleans the DataFrame by removing unwanted characters from each column.\n",
    "        \"\"\"\n",
    "        # Replace '\\n' with space (or '') in all text columns\n",
    "        string_columns = df.select_dtypes(include=['object']).columns\n",
    "        df[string_columns] = df[string_columns].apply(lambda x: x.str.replace('\\n', ' ', regex=True))\n",
    "        return df\n",
    "\n",
    "    def save_cleaned_data(self, df):\n",
    "        \"\"\"\n",
    "        Saves the cleaned DataFrame to a new CSV file in the root directory.\n",
    "        \"\"\"\n",
    "        cleaned_file_path = self.config.root_dir / \"cleaned_data.csv\"\n",
    "        df.to_csv(cleaned_file_path, index=False)\n",
    "        print(f\"Cleaned data saved to {cleaned_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-07 05:56:57,642: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2024-04-07 05:56:57,664: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-04-07 05:56:57,679: INFO: common: created directory at: artifacts]\n",
      "[2024-04-07 05:56:57,706: INFO: common: created directory at: artifacts/data_preparation]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/admin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/admin/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /Users/admin/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                                URL  \\\n",
      "0           0  http://www.leathermag.com/news/newsstahl-expan...   \n",
      "1           1  http://www.leathermag.com/news/newsbirkenstock...   \n",
      "2           2  http://www.leathermag.com/news/newsmacys-inc-r...   \n",
      "3           3  http://www.leathermag.com/news/newseu-cracks-d...   \n",
      "4           4  http://www.leathermag.com/news/newsburberry-ad...   \n",
      "\n",
      "                                               Title  \\\n",
      "0      Stahl expands ZDHC level 3certified portfolio   \n",
      "1  Birkenstock Posts 1.492 Billion Revenues in Fi...   \n",
      "2  Macys Inc. Rejects Unsolicited Proposal from A...   \n",
      "3  EU Cracks Down on Greenwashing and Misleading ...   \n",
      "4  Burberry Adjusts Financial Outlook Amidst Luxu...   \n",
      "\n",
      "                                        TitleAndDate  \\\n",
      "0   Stahl expands ZDHC level 3-certified portfoli...   \n",
      "1   Birkenstock Posts €1.492 Billion Revenues in ...   \n",
      "2   Macy’s, Inc. Rejects Unsolicited Proposal fro...   \n",
      "3   EU Cracks Down on Greenwashing and Misleading...   \n",
      "4   Burberry Adjusts Financial Outlook Amidst Lux...   \n",
      "\n",
      "                                               Texts  \n",
      "0   Stahl the world leader in speciality coatings...  \n",
      "1   Birkenstock a prominent Germanybased global f...  \n",
      "2   Macys has officially acknowledged the receipt...  \n",
      "3   Parliament has granted final approval to a di...  \n",
      "4   In recent update Burberry remains confident i...  \n",
      "Normalized text saved to artifacts/data_preparation/normalized_text.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    \n",
    "    config = ConfigurationManager()\n",
    "    data_preparation_config = config.get_data_preparation_config()\n",
    "    data_preparation = DataPreparation(config=data_preparation_config)\n",
    "    df = data_preparation.load_data()\n",
    "    \n",
    "  \n",
    "    \n",
    "except Exception as e:\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "class DataPreparation:\n",
    "    def __init__(self, config: DataPreparationConfig):\n",
    "        self.config = config\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('wordnet')\n",
    "        nltk.download('stopwords')\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Loads the dataset using pandas from the local data file path specified in the configuration.\"\"\"\n",
    "        df = pd.read_csv(self.config.local_data_file)\n",
    "        df = self.clean_data(df)\n",
    "        print(df.head(5))\n",
    "        self.normalize_and_save(df)\n",
    "        return df\n",
    "\n",
    "    def clean_data(self, df):\n",
    "        \"\"\"Cleans the DataFrame by removing unwanted characters from each column.\"\"\"\n",
    "        string_columns = df.select_dtypes(include=['object']).columns\n",
    "        df[string_columns] = df[string_columns].apply(lambda x: x.str.replace('\\n', ' ', regex=True))\n",
    "        return df\n",
    "\n",
    "    def normalize_and_save(self, df):\n",
    "        \"\"\"Aggregates text from the DataFrame, normalizes it, and saves to a text file.\"\"\"\n",
    "        # Aggregate text from all columns into a single string\n",
    "        aggregated_text = \" \".join(df.astype(str).sum(axis=1))\n",
    "        normalized_text = self.normalize(aggregated_text)\n",
    "        normalized_file_path = self.config.root_dir / \"normalized_text.txt\"\n",
    "        with open(normalized_file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(normalized_text)\n",
    "        print(f\"Normalized text saved to {normalized_file_path}\")\n",
    "\n",
    "    def normalize(self, text):\n",
    "        \"\"\"Applies text normalization including lowercase conversion, punctuation removal, stopword removal, and lemmatization.\"\"\"\n",
    "        words = word_tokenize(text)\n",
    "        words = [word.lower() for word in words if word.isalpha()]\n",
    "        words = [word for word in words if word not in self.stop_words]\n",
    "        words = [self.lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "        return ' '.join(words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
